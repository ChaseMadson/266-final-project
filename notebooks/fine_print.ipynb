{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Title]\n",
    "\n",
    "[Description]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from random import sample, seed\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Load Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Data into Training and Test Sets (Based on Policies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Data\n",
    "data = pd.read_csv('../data.csv')\n",
    "\n",
    "# Set the seed to fix the samples in place\n",
    "seed(0)\n",
    "\n",
    "splits = {'training':75,'val':20,'test':20}\n",
    "# splits = {'training':28,'val':8,'test':4}\n",
    "\n",
    "# Sample 75 of the 115 unique policy_id values, then save the other 40 for test\n",
    "policies_for_training = sample(list(set(data.policy_id)), splits['training'])\n",
    "train = data[data.policy_id.isin(policies_for_training)]\n",
    "train = list(train.drop(['policy_id', 'segment_id'], axis=1).itertuples(index=False, name=None))\n",
    "train_segments = [row[0] for row in train]\n",
    "train_flags = [(row[1], row[2], row[3], row[4]) for row in train]\n",
    "\n",
    "# # Sample 20 of the remaining for validation\n",
    "policies_for_val = sample(list(set(data[~data.policy_id.isin(policies_for_training)].policy_id)), splits['val'])\n",
    "val = data[data.policy_id.isin(policies_for_val)]\n",
    "val = list(val.drop(['policy_id', 'segment_id'], axis=1).itertuples(index=False, name=None))\n",
    "val_segments = [row[0] for row in val]\n",
    "val_flags = [(row[1], row[2], row[3], row[4]) for row in val]\n",
    "\n",
    "# # Sample 20 of the remaining for test\n",
    "test = data[~data.policy_id.isin(policies_for_training+policies_for_val)]\n",
    "test = list(test.drop(['policy_id', 'segment_id'], axis=1).itertuples(index=False, name=None))\n",
    "test_segments = [row[0] for row in test]\n",
    "test_flags = [(row[1], row[2], row[3], row[4]) for row in test]\n",
    "\n",
    "del data, policies_for_training, train, val, test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = 'nlpaueb/legal-bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModel.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the dataset, truncate when passed `max_length`, \n",
    "# and pad with 0's when less than `max_length` and return a tf Tensor\n",
    "train_encodings = tokenizer(\n",
    "    train_segments,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=max_length,\n",
    "    return_tensors='tf'\n",
    ")\n",
    "\n",
    "val_encodings = tokenizer(\n",
    "    val_segments, \n",
    "    truncation=True, \n",
    "    padding=True, \n",
    "    max_length=max_length, \n",
    "    return_tensors='tf'\n",
    ")\n",
    "\n",
    "test_encodings = tokenizer(\n",
    "    test_segments, \n",
    "    truncation=True, \n",
    "    padding=True, \n",
    "    max_length=max_length, \n",
    "    return_tensors='tf'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST ZONE STARTS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from transformers import TFBertModel\n",
    "\n",
    "def create_model(\n",
    "        checkpoint=model_checkpoint,\n",
    "        num_labels=1,\n",
    "        hidden_size=201, \n",
    "        dropout=0.5,\n",
    "        learning_rate=0.0001\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Build a simple classification model with BERT. Use the Pooler Output for classification purposes.\n",
    "    \"\"\"\n",
    "    bert_model = TFBertModel.from_pretrained(checkpoint)                                              \n",
    "\n",
    "    # Train all layers in BERT\n",
    "    bert_model.trainable = True\n",
    "\n",
    "    # Define the BERT inputs\n",
    "    input_ids = Input(shape=(max_length,), dtype=tf.int64, name='input_ids_layer')\n",
    "    token_type_ids = Input(shape=(max_length,), dtype=tf.int64, name='token_type_ids_layer')\n",
    "    attention_mask = Input(shape=(max_length,), dtype=tf.int64, name='attention_mask_layer')\n",
    "    bert_inputs = {\n",
    "        'input_ids': input_ids,\n",
    "        'token_type_ids': token_type_ids,\n",
    "        'attention_mask': attention_mask\n",
    "    }\n",
    "\n",
    "    # Define the pooler output from BERT to pass to our hidden layer\n",
    "    # bert_out = bert_model(bert_inputs)\n",
    "    # pooler_token = bert_out[1]\n",
    "    pooler_token = bert_model(bert_inputs)[1]\n",
    "\n",
    "    # Create a hidden size layer of size 201\n",
    "    hidden = Dense(\n",
    "        units=hidden_size,\n",
    "        activation='relu',\n",
    "        name='hidden_layer'\n",
    "    )(pooler_token)\n",
    "\n",
    "    # Add a dropout layer\n",
    "    hidden = Dropout(dropout)(hidden)\n",
    "\n",
    "    # Output 1\n",
    "    classification1 = Dense(\n",
    "        units=num_labels,\n",
    "        activation='sigmoid',\n",
    "        name='first_party_data_output'\n",
    "    )(hidden)\n",
    "\n",
    "    # Output 2\n",
    "    classification2 = Dense(\n",
    "        units=num_labels,\n",
    "        activation='sigmoid',\n",
    "        name='third_party_sharing_output'\n",
    "    )(hidden)\n",
    "    \n",
    "    # Output 3\n",
    "    classification3 = Dense(\n",
    "        units=num_labels,\n",
    "        activation='sigmoid',\n",
    "        name='first_party_choice_output'\n",
    "    )(hidden)\n",
    "\n",
    "    # Output 4\n",
    "    classification4 = Dense(\n",
    "        units=num_labels,\n",
    "        activation='sigmoid',\n",
    "        name='third_party_choice_output'\n",
    "    )(hidden)\n",
    "    \n",
    "    classification_model = keras.Model(\n",
    "        inputs=[input_ids, token_type_ids, attention_mask], \n",
    "        outputs={\n",
    "            'first_party_data_output': classification1,\n",
    "            'third_party_sharing_output': classification2,\n",
    "            'first_party_choice_output': classification3,\n",
    "            'third_party_choice_output': classification4,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    classification_model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss='binary_crossentropy', \n",
    "        metrics='accuracy'\n",
    "    )\n",
    "\n",
    "    return classification_model\n",
    "\n",
    "bert_model = create_model()\n",
    "bert_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Move above\n",
    "train_flags2 = {\n",
    "    'first_party_data_output': tf.convert_to_tensor([x[0] for x in train_flags]),\n",
    "    'third_party_sharing_output': tf.convert_to_tensor([x[1] for x in train_flags]),\n",
    "    'first_party_choice_output': tf.convert_to_tensor([x[2] for x in train_flags]),\n",
    "    'third_party_choice_output': tf.convert_to_tensor([x[3] for x in train_flags]),\n",
    "    }\n",
    "val_flags2 = {\n",
    "        'first_party_data_output': tf.convert_to_tensor([x[0] for x in val_flags]),\n",
    "        'third_party_sharing_output': tf.convert_to_tensor([x[1] for x in val_flags]),\n",
    "        'first_party_choice_output': tf.convert_to_tensor([x[2] for x in val_flags]),\n",
    "        'third_party_choice_output': tf.convert_to_tensor([x[3] for x in val_flags]),\n",
    "        }\n",
    "test_flags2 = {\n",
    "        'first_party_data_output': tf.convert_to_tensor([x[0] for x in test_flags]),\n",
    "        'third_party_sharing_output': tf.convert_to_tensor([x[1] for x in test_flags]),\n",
    "        'first_party_choice_output': tf.convert_to_tensor([x[2] for x in test_flags]),\n",
    "        'third_party_choice_output': tf.convert_to_tensor([x[3] for x in test_flags]),\n",
    "        }\n",
    "\n",
    "bert_model_history = bert_model.fit(\n",
    "    [train_encodings.input_ids, train_encodings.token_type_ids, train_encodings.attention_mask], \n",
    "    train_flags2,\n",
    "    \n",
    "    validation_data=(\n",
    "        [val_encodings.input_ids, val_encodings.token_type_ids, val_encodings.attention_mask], \n",
    "        val_flags2\n",
    "        \n",
    "    ),    \n",
    "    batch_size=12,\n",
    "    epochs=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = bert_model.evaluate([\n",
    "    test_encodings.input_ids, \n",
    "    test_encodings.token_type_ids, \n",
    "    test_encodings.attention_mask\n",
    "    ],\n",
    "    test_flags2\n",
    ")\n",
    "\n",
    "print('Test loss:', round(score[0], 5)) \n",
    "print('Test accuracy:', round(score[1], 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = bert_model.predict([\n",
    "    test_encodings.input_ids, \n",
    "    test_encodings.token_type_ids, \n",
    "    test_encodings.attention_mask\n",
    "])\n",
    "\n",
    "threshold = 0.05\n",
    "predictions2 = [(a.item() > threshold, b.item() > threshold, c.item() > threshold, d.item() > threshold) for a, b, c, d \n",
    "                in zip(predictions['first_party_data_output'],\n",
    "                        predictions['third_party_sharing_output'],\n",
    "                        predictions['first_party_choice_output'],\n",
    "                        predictions['third_party_choice_output'])]\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(\n",
    "    test_flags,\n",
    "    predictions2\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "w266-final-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
