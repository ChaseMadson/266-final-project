{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Title]\n",
    "\n",
    "[Description]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chase\\.conda\\envs\\w266-final-project\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import sample, seed\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import TFBertModel, AutoTokenizer\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Load Data\n",
    "\n",
    "Data is read in as it was produced by the *data_processing.ipynb* notebook.\n",
    "\n",
    "Data is split into training, validation, and test sets. To avoid training data being leaked into the test or validation sets that could occur when different segments of the same policy appear in multiple sets, the split is performed at the policy level (75 policies in training, the remaining split evenly into test and validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Data\n",
    "data = pd.read_csv('../data.csv')#.sample(100)\n",
    "\n",
    "# Set the seed to fix the samples in place\n",
    "seed(0)\n",
    "\n",
    "splits = {'training':75,'val':20,'test':20}\n",
    "# splits = {'training':28,'val':8,'test':4}\n",
    "\n",
    "# Sample 75 of the 115 unique policy_id values, then save the other 40 for test\n",
    "policies_for_training = sample(list(set(data.policy_id)), splits['training'])\n",
    "train = data[data.policy_id.isin(policies_for_training)]\n",
    "train = list(train.drop(['policy_id', 'segment_id'], axis=1).itertuples(index=False, name=None))\n",
    "train_segments = [row[0] for row in train]\n",
    "train_flags = [(row[1], row[2], row[3], row[4]) for row in train]\n",
    "train_flags = {\n",
    "    'first_party_data_output': tf.convert_to_tensor([x[0] for x in train_flags]),\n",
    "    'third_party_sharing_output': tf.convert_to_tensor([x[1] for x in train_flags]),\n",
    "    'first_party_choice_output': tf.convert_to_tensor([x[2] for x in train_flags]),\n",
    "    'third_party_choice_output': tf.convert_to_tensor([x[3] for x in train_flags]),\n",
    "    }\n",
    "\n",
    "# # Sample 20 of the remaining for validation\n",
    "policies_for_val = sample(list(set(data[~data.policy_id.isin(policies_for_training)].policy_id)), splits['val'])\n",
    "val = data[data.policy_id.isin(policies_for_val)]\n",
    "val = list(val.drop(['policy_id', 'segment_id'], axis=1).itertuples(index=False, name=None))\n",
    "val_segments = [row[0] for row in val]\n",
    "val_flags = [(row[1], row[2], row[3], row[4]) for row in val]\n",
    "val_flags = {\n",
    "        'first_party_data_output': tf.convert_to_tensor([x[0] for x in val_flags]),\n",
    "        'third_party_sharing_output': tf.convert_to_tensor([x[1] for x in val_flags]),\n",
    "        'first_party_choice_output': tf.convert_to_tensor([x[2] for x in val_flags]),\n",
    "        'third_party_choice_output': tf.convert_to_tensor([x[3] for x in val_flags]),\n",
    "        }\n",
    "\n",
    "# # Sample 20 of the remaining for test\n",
    "test = data[~data.policy_id.isin(policies_for_training+policies_for_val)]\n",
    "test = list(test.drop(['policy_id', 'segment_id'], axis=1).itertuples(index=False, name=None))\n",
    "test_segments = [row[0] for row in test]\n",
    "test_flags_simple = [(row[1], row[2], row[3], row[4]) for row in test]\n",
    "test_flags = {\n",
    "        'first_party_data_output': tf.convert_to_tensor([x[0] for x in test_flags_simple]),\n",
    "        'third_party_sharing_output': tf.convert_to_tensor([x[1] for x in test_flags_simple]),\n",
    "        'first_party_choice_output': tf.convert_to_tensor([x[2] for x in test_flags_simple]),\n",
    "        'third_party_choice_output': tf.convert_to_tensor([x[3] for x in test_flags_simple]),\n",
    "        }\n",
    "\n",
    "del data, policies_for_training, train, val, test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding hyperparameters\n",
    "MODEL_CHECKPOINT = 'nlpaueb/legal-bert-base-uncased' # Only uncased is available\n",
    "MAX_LENGTH = 150\n",
    "\n",
    "# Model hyperparameters\n",
    "HIDDEN_LAYER_SIZE = 256\n",
    "DROPOUT_RATE = 1/3\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Train-time hyperparameters\n",
    "BATCH_SIZE = 10\n",
    "EPOCHS = 3\n",
    "\n",
    "# Classification hyperparameters\n",
    "THRESHOLD = 0.025"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Tokenize Text from Policy Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "# tokenize the dataset, truncate when passed `MAX_LENGTH`, \n",
    "# and pad with 0's when less than `MAX_LENGTH` and return a tf Tensor\n",
    "tr_encodings = tokenizer(\n",
    "    train_segments,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=MAX_LENGTH,\n",
    "    return_tensors='tf'\n",
    ")\n",
    "\n",
    "val_encodings = tokenizer(\n",
    "    val_segments, \n",
    "    truncation=True, \n",
    "    padding=True, \n",
    "    max_length=MAX_LENGTH, \n",
    "    return_tensors='tf'\n",
    ")\n",
    "\n",
    "test_encodings = tokenizer(\n",
    "    test_segments, \n",
    "    truncation=True, \n",
    "    padding=True, \n",
    "    max_length=MAX_LENGTH, \n",
    "    return_tensors='tf'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at nlpaueb/legal-bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at nlpaueb/legal-bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " attention_mask_layer (InputLay  [(None, 150)]       0           []                               \n",
      " er)                                                                                              \n",
      "                                                                                                  \n",
      " input_ids_layer (InputLayer)   [(None, 150)]        0           []                               \n",
      "                                                                                                  \n",
      " token_type_ids_layer (InputLay  [(None, 150)]       0           []                               \n",
      " er)                                                                                              \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['attention_mask_layer[0][0]',   \n",
      "                                thPoolingAndCrossAt               'input_ids_layer[0][0]',        \n",
      "                                tentions(last_hidde               'token_type_ids_layer[0][0]']   \n",
      "                                n_state=(None, 150,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " hidden_layer (Dense)           (None, 256)          196864      ['tf_bert_model[0][1]']          \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)           (None, 256)          0           ['hidden_layer[0][0]']           \n",
      "                                                                                                  \n",
      " first_party_choice_output (Den  (None, 1)           257         ['dropout_37[0][0]']             \n",
      " se)                                                                                              \n",
      "                                                                                                  \n",
      " first_party_data_output (Dense  (None, 1)           257         ['dropout_37[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " third_party_choice_output (Den  (None, 1)           257         ['dropout_37[0][0]']             \n",
      " se)                                                                                              \n",
      "                                                                                                  \n",
      " third_party_sharing_output (De  (None, 1)           257         ['dropout_37[0][0]']             \n",
      " nse)                                                                                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,680,132\n",
      "Trainable params: 109,680,132\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model(\n",
    "        checkpoint=MODEL_CHECKPOINT,\n",
    "        hidden_size=HIDDEN_LAYER_SIZE, \n",
    "        dropout=DROPOUT_RATE,\n",
    "        learning_rate=LEARNING_RATE\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Build a multi-label classification model using Legal-BERT\n",
    "    One output head per binary label we wish to classify\n",
    "    \"\"\"\n",
    "    bert_model = TFBertModel.from_pretrained(checkpoint)                                              \n",
    "\n",
    "    # Train all layers in BERT\n",
    "    bert_model.trainable = True\n",
    "\n",
    "    # Define the BERT inputs\n",
    "    input_ids = Input(shape=(MAX_LENGTH,), dtype=tf.int64, name='input_ids_layer')\n",
    "    token_type_ids = Input(shape=(MAX_LENGTH,), dtype=tf.int64, name='token_type_ids_layer')\n",
    "    attention_mask = Input(shape=(MAX_LENGTH,), dtype=tf.int64, name='attention_mask_layer')\n",
    "    bert_inputs = {\n",
    "        'input_ids': input_ids,\n",
    "        'token_type_ids': token_type_ids,\n",
    "        'attention_mask': attention_mask\n",
    "    }\n",
    "\n",
    "    # Define the pooler output from BERT to pass to our hidden layer\n",
    "    pooler_token = bert_model(bert_inputs)[1]\n",
    "\n",
    "    # Create a hidden layer\n",
    "    hidden = Dense(\n",
    "        units=hidden_size, activation='relu', name='hidden_layer'\n",
    "    )(pooler_token)\n",
    "\n",
    "    # Add a dropout layer\n",
    "    hidden = Dropout(dropout)(hidden)\n",
    "\n",
    "    # Output layers for each of the 4 labels\n",
    "    labs = list(train_flags.keys())\n",
    "    outputs = {}\n",
    "\n",
    "    for i in range(len(labs)):\n",
    "        outputs[labs[i]] = Dense(units=1, activation='sigmoid', name=labs[i])(hidden)\n",
    "\n",
    "    classification_model = keras.Model(\n",
    "        inputs=[input_ids, token_type_ids, attention_mask], \n",
    "        outputs=outputs\n",
    "    )\n",
    "    \n",
    "    classification_model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss='binary_crossentropy', \n",
    "        metrics='accuracy'\n",
    "    )\n",
    "\n",
    "    return classification_model\n",
    "\n",
    "bert_model = create_model()\n",
    "bert_model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "266/266 [==============================] - 1307s 5s/step - loss: 1.2917 - first_party_choice_output_loss: 0.2161 - first_party_data_output_loss: 0.4902 - third_party_choice_output_loss: 0.1210 - third_party_sharing_output_loss: 0.4644 - first_party_choice_output_accuracy: 0.9503 - first_party_data_output_accuracy: 0.8249 - third_party_choice_output_accuracy: 0.9778 - third_party_sharing_output_accuracy: 0.8433 - val_loss: 1.1990 - val_first_party_choice_output_loss: 0.2549 - val_first_party_data_output_loss: 0.4302 - val_third_party_choice_output_loss: 0.1012 - val_third_party_sharing_output_loss: 0.4126 - val_first_party_choice_output_accuracy: 0.9342 - val_first_party_data_output_accuracy: 0.8457 - val_third_party_choice_output_accuracy: 0.9794 - val_third_party_sharing_output_accuracy: 0.8580\n",
      "Epoch 2/3\n",
      "266/266 [==============================] - 1266s 5s/step - loss: 1.2474 - first_party_choice_output_loss: 0.2143 - first_party_data_output_loss: 0.4751 - third_party_choice_output_loss: 0.1161 - third_party_sharing_output_loss: 0.4419 - first_party_choice_output_accuracy: 0.9529 - first_party_data_output_accuracy: 0.8331 - third_party_choice_output_accuracy: 0.9785 - third_party_sharing_output_accuracy: 0.8505 - val_loss: 1.3026 - val_first_party_choice_output_loss: 0.2628 - val_first_party_data_output_loss: 0.4306 - val_third_party_choice_output_loss: 0.1018 - val_third_party_sharing_output_loss: 0.5074 - val_first_party_choice_output_accuracy: 0.9342 - val_first_party_data_output_accuracy: 0.8457 - val_third_party_choice_output_accuracy: 0.9794 - val_third_party_sharing_output_accuracy: 0.8580\n",
      "Epoch 3/3\n",
      "266/266 [==============================] - 1267s 5s/step - loss: 1.2460 - first_party_choice_output_loss: 0.2116 - first_party_data_output_loss: 0.4685 - third_party_choice_output_loss: 0.1240 - third_party_sharing_output_loss: 0.4419 - first_party_choice_output_accuracy: 0.9525 - first_party_data_output_accuracy: 0.8328 - third_party_choice_output_accuracy: 0.9785 - third_party_sharing_output_accuracy: 0.8501 - val_loss: 1.1844 - val_first_party_choice_output_loss: 0.2444 - val_first_party_data_output_loss: 0.4302 - val_third_party_choice_output_loss: 0.1006 - val_third_party_sharing_output_loss: 0.4091 - val_first_party_choice_output_accuracy: 0.9342 - val_first_party_data_output_accuracy: 0.8457 - val_third_party_choice_output_accuracy: 0.9794 - val_third_party_sharing_output_accuracy: 0.8580\n"
     ]
    }
   ],
   "source": [
    "bert_model_history = bert_model.fit(\n",
    "    [tr_encodings.input_ids, tr_encodings.token_type_ids, tr_encodings.attention_mask], \n",
    "    train_flags,\n",
    "    validation_data=(\n",
    "        [val_encodings.input_ids, val_encodings.token_type_ids, val_encodings.attention_mask], \n",
    "        val_flags\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VII. Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 79s 4s/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.15      1.00      0.26        97\n",
      "           1       0.15      1.00      0.27       100\n",
      "           2       0.04      1.00      0.07        23\n",
      "           3       0.00      0.00      0.00        12\n",
      "\n",
      "   micro avg       0.11      0.95      0.20       232\n",
      "   macro avg       0.08      0.75      0.15       232\n",
      "weighted avg       0.13      0.95      0.23       232\n",
      " samples avg       0.11      0.30      0.16       232\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chase\\.conda\\envs\\w266-final-project\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\chase\\.conda\\envs\\w266-final-project\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "predictions = bert_model.predict([\n",
    "    test_encodings.input_ids, \n",
    "    test_encodings.token_type_ids, \n",
    "    test_encodings.attention_mask\n",
    "])\n",
    "\n",
    "predictions = [\n",
    "    (a.item() > THRESHOLD, b.item() > THRESHOLD, c.item() > THRESHOLD, d.item() > THRESHOLD) \n",
    "    for a, b, c, d in zip(\n",
    "        predictions['first_party_data_output'],\n",
    "        predictions['third_party_sharing_output'],\n",
    "        predictions['first_party_choice_output'],\n",
    "        predictions['third_party_choice_output'])\n",
    "]\n",
    "\n",
    "print(classification_report(\n",
    "    test_flags_simple,\n",
    "    predictions\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 77s 4s/step - loss: 1.0982 - first_party_choice_output_loss: 0.1561 - first_party_data_output_loss: 0.4213 - third_party_choice_output_loss: 0.0919 - third_party_sharing_output_loss: 0.4289 - first_party_choice_output_accuracy: 0.9647 - first_party_data_output_accuracy: 0.8510 - third_party_choice_output_accuracy: 0.9816 - third_party_sharing_output_accuracy: 0.8464\n",
      "Test loss: 1.09825\n",
      "Test accuracy: 0.1561\n"
     ]
    }
   ],
   "source": [
    "score = bert_model.evaluate([\n",
    "    test_encodings.input_ids, \n",
    "    test_encodings.token_type_ids, \n",
    "    test_encodings.attention_mask\n",
    "    ],\n",
    "    test_flags\n",
    ")\n",
    "\n",
    "print('Test loss:', round(score[0], 5)) \n",
    "print('Test accuracy:', round(score[1], 5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "w266-final-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
